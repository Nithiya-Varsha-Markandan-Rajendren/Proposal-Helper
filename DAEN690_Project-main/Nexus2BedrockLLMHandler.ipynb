{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c96d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime, timedelta, timezone\n",
    "\n",
    "\n",
    "# Defaults\n",
    "DEFAULT_MODEL_ID = os.environ.get(\"DEFAULT_MODEL_ID\",\"anthropic.claude-instant-v1\")\n",
    "AWS_REGION = os.environ[\"AWS_REGION_OVERRIDE\"] if \"AWS_REGION_OVERRIDE\" in os.environ else os.environ[\"AWS_REGION\"]\n",
    "ENDPOINT_URL = os.environ.get(\"ENDPOINT_URL\", f'https://bedrock-runtime.{AWS_REGION}.amazonaws.com')\n",
    "DEFAULT_MAX_TOKENS = 256\n",
    "\n",
    "# global variables - avoid creating a new client for every request\n",
    "client = None\n",
    "\n",
    "def get_client():\n",
    "    print(\"Connecting to Bedrock Service: \", ENDPOINT_URL)\n",
    "    client = boto3.client(service_name='bedrock-runtime', region_name=AWS_REGION, endpoint_url=ENDPOINT_URL)\n",
    "    return client\n",
    "\n",
    "def get_request_body(modelId, parameters, prompt):\n",
    "    provider = modelId.split(\".\")[0]\n",
    "    request_body = None\n",
    "    if provider == \"anthropic\":\n",
    "        # claude-3 models use new messages format\n",
    "        if modelId.startswith(\"anthropic.claude-3\"):\n",
    "            request_body = {\n",
    "                \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "                \"messages\": [{\"role\": \"user\", \"content\": [{'type':'text','text': prompt}]}],\n",
    "                \"max_tokens\": DEFAULT_MAX_TOKENS\n",
    "            }\n",
    "            request_body.update(parameters)\n",
    "        else:\n",
    "            request_body = {\n",
    "                \"prompt\": prompt,\n",
    "                \"max_tokens_to_sample\": DEFAULT_MAX_TOKENS\n",
    "            } \n",
    "            request_body.update(parameters)\n",
    "    elif provider == \"ai21\":\n",
    "        request_body = {\n",
    "            \"prompt\": prompt,\n",
    "            \"maxTokens\": DEFAULT_MAX_TOKENS\n",
    "        }\n",
    "        request_body.update(parameters)\n",
    "    elif provider == \"amazon\":\n",
    "        textGenerationConfig = {\n",
    "            \"maxTokenCount\": DEFAULT_MAX_TOKENS\n",
    "        }\n",
    "        textGenerationConfig.update(parameters)\n",
    "        request_body = {\n",
    "            \"inputText\": prompt,\n",
    "            \"textGenerationConfig\": textGenerationConfig\n",
    "        }\n",
    "    elif provider == \"cohere\":\n",
    "        request_body = {\n",
    "            \"prompt\": prompt,\n",
    "            \"max_tokens\": DEFAULT_MAX_TOKENS\n",
    "        }\n",
    "        request_body.update(parameters)\n",
    "    elif provider == \"meta\":\n",
    "        request_body = {\n",
    "            \"prompt\": prompt,\n",
    "            \"max_gen_len\": DEFAULT_MAX_TOKENS\n",
    "        }\n",
    "        request_body.update(parameters)\n",
    "    else:\n",
    "        raise Exception(\"Unsupported provider: \", provider)\n",
    "    return request_body\n",
    "\n",
    "def get_generate_text(modelId, response):\n",
    "    provider = modelId.split(\".\")[0]\n",
    "    generated_text = None\n",
    "    response_body = json.loads(response.get(\"body\").read())\n",
    "    print(\"Response body: \", json.dumps(response_body))\n",
    "    if provider == \"anthropic\":\n",
    "        # claude-3 models use new messages format\n",
    "        if modelId.startswith(\"anthropic.claude-3\"):\n",
    "            generated_text = response_body.get(\"content\")[0].get(\"text\")\n",
    "        else:\n",
    "            generated_text = response_body.get(\"completion\")\n",
    "    elif provider == \"ai21\":\n",
    "        generated_text = response_body.get(\"completions\")[0].get(\"data\").get(\"text\")\n",
    "    elif provider == \"amazon\":\n",
    "        generated_text = response_body.get(\"results\")[0].get(\"outputText\")\n",
    "    elif provider == \"cohere\":\n",
    "        generated_text = response_body.get(\"generations\")[0].get(\"text\")\n",
    "    elif provider == \"meta\":\n",
    "        generated_text = response_body.get(\"generation\")\n",
    "    else:\n",
    "        raise Exception(\"Unsupported provider: \", provider)\n",
    "    return generated_text\n",
    "\n",
    "def call_llm(parameters, prompt):\n",
    "    global client\n",
    "    modelId = parameters.pop(\"modelId\", DEFAULT_MODEL_ID)\n",
    "    body = get_request_body(modelId, parameters, prompt)\n",
    "    print(\"ModelId\", modelId, \"-  Body: \", body)\n",
    "    if (client is None):\n",
    "        client = get_client()\n",
    "    response = client.invoke_model(body=json.dumps(body), modelId=modelId, accept='application/json', contentType='application/json')\n",
    "    generated_text = get_generate_text(modelId, response)\n",
    "    return generated_text\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Example Test Event:\n",
    "{\n",
    "  \"prompt\": \"\\n\\nHuman:Why is the sky blue?\\n\\nAssistant:\",\n",
    "  \"parameters\": {\n",
    "    \"modelId\": \"anthropic.claude-3-sonnet-20240229-v1:0\",\n",
    "    \"temperature\": 0,\n",
    "    \"system\": \"You are an AI assistant that always answers in ryhming couplets\"\n",
    "  }\n",
    "}\n",
    "For supported parameters for each provider model, see Bedrock docs: https://us-east-1.console.aws.amazon.com/bedrock/home?region=us-east-1#/providers\n",
    "\"\"\"\n",
    "def lambda_handler(event, context):\n",
    "    print(\"Event: \", json.dumps(event))\n",
    "    prompt = event[\"prompt\"]\n",
    "    parameters = event[\"parameters\"]\n",
    "    eastern_offset = timezone(timedelta(hours=-5))\n",
    "    # Format the current date and time in local timezone\n",
    "    current_time = datetime.now(eastern_offset)\n",
    "    current_date = current_time.strftime(\"%Y-%m-%d\")\n",
    "    formatted_time = current_time.strftime(\"%I:%M %p\")\n",
    "    current_day = current_time.strftime(\"%A\")\n",
    "    prompt = f\"\"\"\n",
    "    The current date is {current_date}.\n",
    "    The current time is {formatted_time}.\n",
    "    The current day is {current_day}.\n",
    "\n",
    "    {prompt}\n",
    "    \"\"\"\n",
    "    generated_text = call_llm(parameters, prompt)\n",
    "    print(\"Result:\", json.dumps(generated_text))\n",
    "    return {\n",
    "        'generated_text': generated_text\n",
    "    }"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
